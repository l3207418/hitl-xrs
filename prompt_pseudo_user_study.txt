We are evaluating the information quality of explanations generated by an Explainable Recommender System (XRS).
Originally, our goal was to collect **direct feedback from the target users** to assess the quality of explanations. 
However, in this task, instead of asking the actual users, we ask *you* to simulate the evaluation **as if you were the user**. 
That is, we will provide you with a target user’s gameplay history, and your task is to adopt their **persona** and judge the quality of a recommendation explanation from their perspective.

---

You are asked to evaluate the explanation, broken into the following two dimensions:

---

[Fidelity]

Fidelity refers to whether the explanation accurately reflects the **true attributes** of the recommended item.

Use the following definitions:

- **Hit**: The explanation states the item has a feature, and the item truly has it.
- **False Alarm**: The explanation states the item has a feature, but it does not.
- **Miss**: The explanation omits a feature that the item actually has.
- **Correct Rejection**: The explanation omits a feature that the item truly lacks.

You must return a **fidelity score of either 0 or 1**, based on the following rule:

- **Fidelity = 1** if the explanation contains **only Hit or Correct Rejection** — that is, it is fully consistent with the item's actual attributes.
- **Fidelity = 0** if the explanation contains **any False Alarm or Miss** — that is, it includes even one factual inaccuracy or omission.

---

[Attunement]

Attunement refers to whether the explanation reflects the **user’s actual preferences**.

Use the following definitions:

- **Hit**: The explanation mentions a feature the user likes.
- **False Alarm**: The explanation mentions a feature the user does not like.
- **Miss**: The explanation fails to mention a feature the user likes.
- **Correct Rejection**: The explanation avoids mentioning a feature the user does not care about.

You must return:
- An **attunement score between 0.0 and 1.0**, estimating how well the explanation aligns with the user’s preferences.

---

[Persona Setting]

You must adopt the role of the user. Based on the gameplay history provided, **simulate their preferences**, likes, and dislikes. 

---

[User history]

You must simulate the user's persona based on the following game history:
In the user history, the `rating` field indicates purchase and play status:
- `x` = purchased but not played
- `p` = purchased and played
{user_history_prompt}

---

[Explanation text]

You will evaluate the explanation:
{explanation_text}

---

[Output Instruction]

Return a CSV with columns:
user_id,item_id,item_name,feature_name,feature_value,explanation,fidelity,attunement,reason

Each row must include:
**fidelity: [reason], attunement: [reason]**
You must include both 'fidelity' and 'attunement' keys in the 'reason' object.

---

Now evaluate for:
user_id: {uid}
item_id: {recom_item_id}